{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "# whitespace pretokenizer ?\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "unk_token = \"<UNK>\"\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_WordPieceTrainer(file_list, vocab_size=30_000, min_frequency=5, limit_alphabet=500):\n",
    "    \"\"\"\n",
    "    Train WP tokenizer from a list of files.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=[\n",
    "            \"<UNK>\",\n",
    "            \"<SEP>\",\n",
    "            \"<MASK>\",\n",
    "            \"<CLS>\",\n",
    "        ],\n",
    "        show_progress=True,\n",
    "        limit_alphabet=limit_alphabet\n",
    "    )\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    tokenizer.train(file_list, trainer)\n",
    "    \n",
    "    tokenizer.save(\"./WPtok-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./WP_tok-trained.json\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SentencePieceBPETokenizer(data, vocab_size=30_000, min_frequency=5, limit_alphabet=500):\n",
    "    \"\"\"\n",
    "    trin SP_BPE tokenizer from a list of files.\n",
    "    \"\"\"\n",
    "    tokenizer = SentencePieceBPETokenizer()\n",
    "    tokenizer.train_from_iterator(\n",
    "        data,\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        show_progress=True,\n",
    "        limit_alphabet=limit_alphabet,\n",
    "    )\n",
    "    tokenizer.save(\"./SP_BPE_tok-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./SP_BPE_tok-trained.json\")\n",
    "    return tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SentencePieceUGTokenizer(data, vocab_size=30_000):\n",
    "    \"\"\"\n",
    "    trin SP_UG tokenizer from a list of files.\n",
    "    \"\"\"\n",
    "    tokenizer = SentencePieceUnigramTokenizer()\n",
    "    tokenizer.train_from_iterator(\n",
    "        data,\n",
    "        vocab_size=vocab_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    tokenizer.save(\"./SP_UG_tok-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./SP_UG_tok-trained.json\")\n",
    "    return tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"lore ipsum dolor sit amet consectetur adipiscing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum\"\n",
    "\n",
    "# train_iterator. for unigram UnigramTrainer can be used\n",
    "train_SentencePieceBPETokenizer(data)\n",
    "train_SentencePieceUGTokenizer(data)\n",
    "\n",
    "# train using word piece trainer\n",
    "# train_WordPieceTrainer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f49bcd837b558ff6ff8826d7d17a0064837aa39a4f4484b433957070606ef428"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
