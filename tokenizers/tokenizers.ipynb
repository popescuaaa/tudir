{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer, BpeTrainer, UnigramTrainer\n",
    "# whitespace pretokenizer ?\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "unk_token = \"<UNK>\"\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_WordPieceTrainer(file_list, vocab_size=30_000, min_frequency=5, limit_alphabet=500):\n",
    "    \"\"\"\n",
    "    Train WP tokenizer from a list of files.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=spl_tokens,\n",
    "        show_progress=True,\n",
    "        limit_alphabet=limit_alphabet\n",
    "    )\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    tokenizer.train(file_list, trainer)\n",
    "    \n",
    "    tokenizer.save(\"./WP_tok-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./WP_tok-trained.json\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SentencePieceBPETokenizer(data, vocab_size=30_000, min_frequency=5, limit_alphabet=500):\n",
    "    \"\"\"\n",
    "    trin SP_BPE tokenizer from a list of files.\n",
    "    \"\"\"\n",
    "    tokenizer = SentencePieceBPETokenizer()\n",
    "    tokenizer.train_from_iterator(\n",
    "        data,\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        show_progress=True,\n",
    "        limit_alphabet=limit_alphabet,\n",
    "    )\n",
    "    tokenizer.save(\"./SP_BPE_tok-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./SP_BPE_tok-trained.json\")\n",
    "    return tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SentencePieceUGTokenizer(data, vocab_size=30_000):\n",
    "    \"\"\"\n",
    "    trin SP_UG tokenizer from a list of files.\n",
    "    \"\"\"\n",
    "    tokenizer = SentencePieceUnigramTokenizer()\n",
    "    tokenizer.train_from_iterator(\n",
    "        data,\n",
    "        vocab_size=vocab_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    tokenizer.save(\"./SP_UG_tok-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./SP_UG_tok-trained.json\")\n",
    "    return tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x179ba641820>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = [\"./data.txt\"]\n",
    "\n",
    "data = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\n",
    "\n",
    "train_WordPieceTrainer(file_list)\n",
    "train_SentencePieceBPETokenizer(data)\n",
    "train_SentencePieceUGTokenizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnigramTrainer & BPETrainer DON'T work as trainers for SentencePiece tokenizer\n",
    "\n",
    "def train_SentencePieceBPETokenizer(file_list, vocab_size=30_000, min_frequency=5, limit_alphabet=500):\n",
    "    \"\"\"\n",
    "    Train SP_BP tokenizer from a list of files.\n",
    "    \"\"\"\n",
    "    tokenizer = SentencePieceBPETokenizer(unk_token = unk_token)\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=spl_tokens,\n",
    "        show_progress=True,\n",
    "        limit_alphabet=limit_alphabet\n",
    "    )\n",
    "    tokenizer.train(file_list, trainer)\n",
    "\n",
    "    tokenizer.save(\"./SP_BP_tok-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./SP_BP_tok_tok-trained.json\")\n",
    "    return tokenizer\n",
    "\n",
    "def train_SentencePieceUGTokenizer(file_list, vocab_size=30_000, min_frequency=5, limit_alphabet=500):\n",
    "    \"\"\"\n",
    "    Train SP_UG tokenizer from a list of files.\n",
    "    \"\"\"\n",
    "    tokenizer = SentencePieceUnigramTokenizer()\n",
    "    trainer = UnigramTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=spl_tokens,\n",
    "        show_progress=True,\n",
    "        limit_alphabet=limit_alphabet\n",
    "    )\n",
    "    tokenizer.train(file_list, trainer)\n",
    "\n",
    "    tokenizer.save(\"./SP_UG_tok-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./SP_UG_tok-trained.json\")\n",
    "    return tokenizer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f49bcd837b558ff6ff8826d7d17a0064837aa39a4f4484b433957070606ef428"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
